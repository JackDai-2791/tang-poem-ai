# ğŸ­ Tang Poem AI

A neural language model trained to generate classical Chinese Tang poetry. This project implements a simple bigram language model that learns to generate poetry in the style of Tang dynasty poets.

## ğŸ“ Project Structure

```
tang-poem-ai/
â”œâ”€â”€ train.py          # Training script for the bigram model
â”œâ”€â”€ generate.py       # Basic sampling script
â”œâ”€â”€ sample_model.py   # Interactive sampling interface
â”œâ”€â”€ batch_sample.py   # Batch generation script
â”œâ”€â”€ prepare.py        # Data preparation script
â”œâ”€â”€ model.pt          # Trained model weights
â”œâ”€â”€ meta.pkl          # Character-to-index mappings
â”œâ”€â”€ train.bin         # Preprocessed training data
â”œâ”€â”€ poems.txt         # Original training poems
â””â”€â”€ README.md         # This file
```

## ğŸš€ Quick Start

### Setup Environment
```bash
# Activate virtual environment
source venv/bin/activate

# Install dependencies (if needed)
pip install torch
```

### Generate Poems

#### 1. Quick Sample (`generate.py`)
```bash
python generate.py
```
Generates samples with different starting characters and a random poem.

#### 2. Interactive Sampling (`sample_model.py`)
```bash
python sample_model.py
```
Provides an interactive menu to:
- Generate with custom starting text
- Generate random poems
- Try different starting characters
- Adjust generation parameters

#### 3. Batch Sampling (`batch_sample.py`)
```bash
python batch_sample.py
```
Generates multiple poems with different starting characters for comparison.

## ğŸ¯ Available Starting Characters

The model supports these starting characters (based on training vocabulary):
- **æ˜¥** (Spring) - æ˜¥çœ ä¸è§‰æ™“
- **æœˆ** (Moon) - æ˜æœˆå…‰
- **èŠ±** (Flower) - èŠ±è½çŸ¥å¤šå°‘
- **å¤œ** (Night) - å¤œæ¥é£é›¨å£°
- **é£** (Wind) - é£é›¨å£°
- **é›¨** (Rain) - é›¨å£°
- **æ˜** (Bright) - æ˜æœˆ
- **åºŠ** (Bed) - åºŠå‰æ˜æœˆ
- **ä¸¾** (Raise) - ä¸¾å¤´æœ›æ˜æœˆ
- **ä½** (Lower) - ä½å¤´æ€æ•…ä¹¡
- **æ€** (Think) - æ€æ•…ä¹¡
- **æœ›** (Gaze) - æœ›æ˜æœˆ

## ğŸ§  Model Architecture

- **Type**: Bigram Language Model
- **Architecture**: Simple embedding-based model
- **Training**: 5000 iterations with AdamW optimizer
- **Context**: 8-token window
- **Vocabulary**: 38 unique characters

## ğŸ“Š Training Data

The model was trained on classical Tang poems including:
- "é™å¤œæ€" (Quiet Night Thoughts) by Li Bai
- "æ˜¥æ™“" (Spring Dawn) by Meng Haoran

## ğŸ¨ Sample Outputs

### Starting with "æ˜¥" (Spring):
```
æ˜¥çœ ä¸è§‰æ™“ï¼Œ
ä½å¤´æ€æ•…ä¹¡ã€‚
ä¸¾å¤´æ€æ•…ä¹¡ã€‚
ä½å¤´æœ›æ˜æœˆå…‰ï¼Œ
ä½å¤´æ€æ•…ä¹¡ã€‚
ä¸¾å¤´æœ›æ˜æœˆå…‰ï¼Œ
å¤„å¤„å¤„å¤„é—»å•¼
```

### Starting with "æœˆ" (Moon):
```
æœˆå…‰ï¼Œ
æ˜¥çœ ä¸è§‰æ™“ï¼Œ
ä½å¤´æ€æ•…ä¹¡ã€‚
ç–‘æ˜¯åœ°ä¸Šéœœã€‚
ä½å¤´æœ›æ˜æœˆå…‰ï¼Œ
èŠ±è½çŸ¥å¤šå°‘ã€‚
èŠ±è½çŸ¥å¤šå°‘ã€‚
æ˜¥çœ 
```

### Random Generation:
```
èŠ±è½çŸ¥å¤šå°‘ã€‚
å¤„é—»å•¼é¸Ÿã€‚
å¤œæ¥é£é›¨å£°ï¼Œ
å¤œæ¥é£é›¨å£°ï¼Œ
ä¸¾å¤´æ€æ•…ä¹¡ã€‚
ä¸¾å¤´æ€æ•…ä¹¡ã€‚
èŠ±è½çŸ¥å¤šå°‘ã€‚
æ˜¥
```

## ğŸ”§ Customization

### Adjusting Generation Parameters

In any of the sampling scripts, you can modify:
- `max_new_tokens`: Number of characters to generate (default: 100)
- `starting_text`: Initial character(s) to start generation
- `temperature`: Sampling temperature (affects randomness)

### Adding New Training Data

1. Add new poems to `poems.txt`
2. Run `python prepare.py` to preprocess
3. Retrain with `python train.py`

## ğŸ“ Usage Examples

### Python API
```python
from generate import sample_model

# Generate with starting character
poem = sample_model("æ˜¥", max_new_tokens=50)
print(poem)

# Generate random poem
random_poem = sample_model(None, max_new_tokens=100)
print(random_poem)
```

### Command Line
```bash
# Quick sample
python generate.py

# Interactive mode
python sample_model.py

# Batch generation
python batch_sample.py
```

## ğŸ¯ Model Performance

The model successfully learns:
- âœ… Classical Chinese punctuation (ï¼Œã€‚)
- âœ… Tang poetry structure and rhythm
- âœ… Thematic coherence (moon, spring, flowers, etc.)
- âœ… Character-level language patterns
- âœ… Poetic line breaks and formatting

## ğŸ”® Future Enhancements

Potential improvements:
- Larger training dataset
- More sophisticated model architecture (Transformer, LSTM)
- Temperature-controlled sampling
- Rhyme and meter constraints
- Multi-line poem generation
- Style transfer capabilities

## ğŸ“š References

- Tang Dynasty Poetry
- Neural Language Modeling
- Character-level Language Models
- Classical Chinese Literature

---

*Generated with â¤ï¸ by Tang Poem AI* 